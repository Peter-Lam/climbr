# Automating the scraping Alitudes booking information based on a cron tab
FROM ubuntu
ENV DOCKER_SCRAPER = true
ENV TZ=America/Toronto
ENV DISPLAY=:99
# Creating working director
RUN mkdir /workspace /workspace/logs/
WORKDIR /workspace/web_scraper

# Install packages and python libraries
RUN apt-get -y update \
    && DEBIAN_FRONTEND=noninteractive apt-get -y install cron curl python3 python3-pip tzdata wget unzip\
    && ln -s /usr/bin/python3 /usr/bin/python \ 
    && ln -s /usr/bin/pip3 /usr/bin/pip \
    && pip install requests selenium pyyaml \ 
    && apt-get update
# Install google chrome
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    && sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list' \
    && apt-get -y update \
    && apt-get install -y google-chrome-stable
# Install chromedriver
RUN wget -O /tmp/chromedriver.zip http://chromedriver.storage.googleapis.com/`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`/chromedriver_linux64.zip \
    && unzip /tmp/chromedriver.zip chromedriver -d /usr/bin/
# Setup cron job
COPY crontab /etc/cron.d/simple-cron
RUN chmod 0644 /etc/cron.d/simple-cron && crontab /etc/cron.d/simple-cron && touch /workspace/logs/cron.log
# Start cron and keep running
CMD cron && tail -f /workspace/logs/cron.log